# DebertaHRM training config for AI2 ARC

defaults:
  - arch: deberta_hrm
  - _self_

hydra:
  output_subdir: null

# Dataset config
dataset_name: allenai/ai2_arc
dataset_config: ARC-Easy
model_name: microsoft/deberta-v3-base
max_length: 512

# Training hyperparameters
global_batch_size: 32
epochs: 10
eval_interval: 1000
checkpoint_every_eval: True

# Learning rates
lr: 3e-5  # Lower LR for fine-tuning
lr_min_ratio: 0.1
lr_warmup_steps: 500

# Standard fine-tuning hyperparameters
beta1: 0.9
beta2: 0.999
weight_decay: 0.01

# No puzzle embeddings for text tasks
puzzle_emb_lr: 0.0
puzzle_emb_weight_decay: 0.0